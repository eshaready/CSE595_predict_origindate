{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97367601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshamunagala/Documents/school/college/grad-sem-1/cse-595/cse595_project/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from model import OrdinalRegressionModel\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589c12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, labels_filename, model_filename, device):\n",
    "    # load label mappings\n",
    "    with open(labels_filename) as f:\n",
    "        maps = json.load(f)\n",
    "\n",
    "    label2id = {k: int(v) for k, v in maps[\"label2id\"].items()}\n",
    "    id2label = {int(k): int(v) for k, v in maps[\"id2label\"].items()}\n",
    "\n",
    "    # recreate model with correct num_classes\n",
    "    num_classes = len(label2id)\n",
    "    model = model_class(model_name=\"distilbert-base-uncased\", num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_filename, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Model loaded with\", num_classes, \"classes.\")\n",
    "    return model, label2id, id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f058301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 25 classes.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, label2id, id2label = load_model(OrdinalRegressionModel, \"labels-20000.json\", \"model-20000.pt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3586cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationTextByYearTestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, decade_to_idx, max_len=128):\n",
    "        self.samples = []\n",
    "        self.has_labels = \"year\" in df.columns and decade_to_idx is not None\n",
    "        self.has_type = \"type\" in df.columns\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            text = str(row['line'])\n",
    "\n",
    "            # removing chunking for this bc then it explodes the test dataset size\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "\n",
    "            sample = {\n",
    "                \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "            if self.has_labels:\n",
    "                decade = (int(row[\"year\"]) // 10) * 10\n",
    "                sample[\"label\"] = torch.tensor(decade_to_idx[decade], dtype=torch.long)\n",
    "            if self.has_type:\n",
    "                sample[\"book\"] = torch.tensor(row[\"type\"] == \"book\", dtype=torch.bool)\n",
    "\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5ee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_label(df, tokenizer, max_len=128, stride=64):\n",
    "    # Clean text\n",
    "    df[\"line\"] = df[\"line\"].astype(str).apply(lambda s: re.sub(r\"\\s+\", \" \", s.lower().strip()))\n",
    "    df = df[df[\"line\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "    # years to decades\n",
    "    df[\"year\"] = (df[\"year\"].astype(int) // 10) * 10\n",
    "    all_decades = sorted(df[\"year\"].unique())\n",
    "    decade_to_idx = {decade: i for i, decade in enumerate(all_decades)}\n",
    "    idx_to_decade = {i: decade for decade, i in decade_to_idx.items()}\n",
    "    df[\"label\"] = df[\"year\"].map(decade_to_idx)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = EvaluationTextByYearTestDataset(df, tokenizer, decade_to_idx, max_len=max_len)\n",
    "    return dataset, len(all_decades), decade_to_idx, idx_to_decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43a2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load test csv as dataloader and do predictions on it \n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "# randomly sampling because there are 516,506 entries in test.csv and thats wayy too many to manageably test on cpu \n",
    "test_df = test_df.sample(n=20000, random_state=12).reset_index(drop=True)\n",
    "test_dataset, num_decades, decade_to_idx, idx_to_decade = preprocess_and_label(test_df, AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93c6ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([  101,  6535,  7249,  8384,  2024,  3098,  5628,  2005,  1996,  4425,\n",
      "         1997,  2367, 21955,  1012,  2012,  1996, 14575,  2057,  2156,  1037,\n",
      "         2554,  2551,  1037, 18112,  1010,  1037,  3573,  1998,  1037,  6501,\n",
      "         2449,  1012,  2178,  2554,  2081,  1037,  3112,  1997,  2551,  1037,\n",
      "        18112,  1010,  1037,  3573,  2007,  2195,  5628,  1010,  1037,  2740,\n",
      "         5427,  2533,  2007,  3174,  7435,  1998,  1037, 21459, 26572, 20464,\n",
      "         5498,  2278,  1010,  5819,  1996,  1000,  1041, 29206, 15876, 14277,\n",
      "         1000,  5324, 21255,  2037, 10336,  1997,  3747, 14678,  1012,  1999,\n",
      "         1996,  4731,  1998,  3760,  4865,  1010,  1996, 26512,  1997,  2522,\n",
      "         1011,  3169,  2024,  2172,  2062, 10975, 12672,  3372,  1012,  1037,\n",
      "         2210, 18112,  2007,  1037,  2200,  2235,  3573,  2003,  1996,  6623,\n",
      "         2927,  1010,  1998,  2130,  2023,  2003,  2025,  2467,  3144,  1012,\n",
      "         2059,  2045,  2024,  1996,  2210,  3182,  2073,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(6), 'book': tensor(True)}, {'input_ids': tensor([  101,  9362,  3793,  1011,  2808,  1010,  4385,  1012,  2410,  4732,\n",
      "         2816,  1012,  3752,  1012,  3793,  1011,  2808,  1010,  2506,  1012,\n",
      "         3449,  3385,  1010,  1059,  1012,  1044,  1012,  1012,  3449,  3385,\n",
      "         3078,  2082,  8141,  1012,  3660,  1010, 18921, 11512,  1004,  2522,\n",
      "         1012,  1005,  7022,  1012,  3976,  1012,  2338,  1015,  1045,  1002,\n",
      "         1014,  1012,  2656,  2338,  1016,  2462,  1012,  3590,  2338,  1017,\n",
      "         3523,  1012,  4029,  2338,  1018,  4921,  1528,  1012,  4029,  3449,\n",
      "         3385,  1010,  1059,  1012,  1044,  1012,  1010,  1998, 17710,  3600,\n",
      "         1010,  1039,  1012,  3449,  3385,  8035,  2082,  8141,  1012,  3660,\n",
      "         1010, 18921, 11512,  1004,  2522,  1012,  1008,  2338,  1015,  1058,\n",
      "         2338,  1016,  1012,  2871,  6819,  2338,  1017,  8890,  2338,  1018,\n",
      "         1012,  4466,  9937,  3449,  3385,  1010,  1059,  1012,  1044,  1012,\n",
      "         1010,  1998,  2448, 11705,  1010,  1048,  1012,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(24), 'book': tensor(True)}, {'input_ids': tensor([ 101, 2129, 2116, 7036, 2111, 2106, 2017, 2681, 2757, 2067, 2045, 1029,\n",
      "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(15), 'book': tensor(False)}, {'input_ids': tensor([  101,  2570,  1016,  8975,  2015,  1012,  2090, 11047,  1012, 11447,\n",
      "         1998,  2621,  1012,  1012,  1012,  1012,  1012,  1012,  2167, 14425,\n",
      "         1998, 24530,  1012,  1012, 21864, 14905,  2100,  1998,  8902,  7011,\n",
      "         2595,  1012,  1012,  1012,  1012,  1012,  1012,  2013,  2000,  3091,\n",
      "         1999,  2519,  1012,  5297,  1012,  1012,  1012,  1012,  1012,  1012,\n",
      "         3028,  3927,  1012,  1012,  1012,  1012,  1012,  4278,  1012,  3963,\n",
      "         9999,  1012,  1012,  1012,  4202,  1012,  1012,  1012,  1012,  1012,\n",
      "         1012,  1050,  1012,  2240,  1041,  1010,  1998,  1059,  8975,  1012,\n",
      "         1012,  1012,  6079,  2475,  1012,  3438,  2167,  2522,  4183,  1012,\n",
      "         1012,  1012,  1012,  1012,  1012,  9645,  2629,  1012,  4002,  5991,\n",
      "         1015,  1998,  1017,  1010, 25312, 25230,  1005,  1055,  5587,  1012,\n",
      "         1012,  3796,  1016,  1010,  6341,  5587,  1012,  1012,  7063,  3927,\n",
      "         1012,  1012,  3000,  3927,  1012,  2148,  2586,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(24), 'book': tensor(True)}, {'input_ids': tensor([ 101, 3398, 1010, 2066, 2017, 1005, 1040, 2022, 1037, 2600, 2044, 2673,\n",
      "        2016, 2253, 2083, 1029,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(15), 'book': tensor(False)}]\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffafe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corn_decode(logits, threshold=0.5):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return torch.sum(probs > threshold, dim=1)\n",
    "\n",
    "def predict_on_test(model, dataloader, id2label, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\", total=len(dataloader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        pred_ids = corn_decode(logits).cpu().tolist()\n",
    "\n",
    "        pred_decades = [id2label[i] for i in pred_ids]\n",
    "        preds.extend(pred_decades)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f25e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting: 100%|██████████| 19842/19842 [15:37<00:00, 21.17it/s]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_on_test(model, test_loader, idx_to_decade, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247bf590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent element (24)'s macro F1: 0.01674092376359941\n",
      "Random choice's macro F1: 0.02882188329460911\n",
      "Model's macro F1: 0.3562043453200865\n",
      "{0: 22, 1: 1, 2: 26, 3: 82, 4: 334, 5: 790, 6: 552, 7: 526, 8: 655, 9: 640, 10: 1114, 11: 399, 12: 246, 13: 386, 14: 7534, 15: 92, 16: 25, 17: 29, 18: 102, 19: 104, 20: 383, 21: 261, 22: 104, 23: 73, 24: 5362}\n",
      "{0: 21, 1: 51, 2: 58, 3: 101, 4: 357, 5: 1003, 6: 684, 7: 491, 8: 1064, 9: 871, 10: 939, 11: 643, 12: 965, 13: 1256, 14: 3216, 15: 1739, 16: 19, 17: 15, 18: 259, 19: 158, 20: 352, 21: 208, 22: 115, 23: 6, 24: 5251}\n"
     ]
    }
   ],
   "source": [
    "true_labels = [int(item[\"label\"].int()) for item in test_dataset]\n",
    "predicted_label_indices = [decade_to_idx[pred] for pred in test_predictions]\n",
    "\n",
    "# MOST FREQUENT ELEMENT BASELINE\n",
    "true_c = collections.Counter(true_labels)\n",
    "mode = true_c.most_common(1)[0][0]\n",
    "most_freq_baseline = [mode for _ in range(len(true_labels))]\n",
    "\n",
    "macro_f1_mode = f1_score(true_labels, most_freq_baseline, average='macro')\n",
    "print(f\"Most frequent element ({mode})'s macro F1:\", macro_f1_mode)\n",
    "\n",
    "# RANDOM CHOICE BASELINE\n",
    "random.seed(12)\n",
    "choices = list(idx_to_decade.keys())\n",
    "random_baseline = random.choices(choices, k=len(true_labels))\n",
    "\n",
    "macro_f1_random = f1_score(true_labels, random_baseline, average='macro')\n",
    "print(f\"Random choice's macro F1:\", macro_f1_random)\n",
    "\n",
    "# MODEL F1\n",
    "macro_f1_predictions = f1_score(true_labels, predicted_label_indices, average='macro')\n",
    "print(\"Model's macro F1:\", macro_f1_predictions)\n",
    "# print(\"\\nFull classification report:\")\n",
    "# print(classification_report(true_labels, predicted_label_indices))\n",
    "print(dict(sorted(collections.Counter(predicted_label_indices).items())))\n",
    "print(dict(sorted(true_c.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632665f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.50201592581393\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(true_labels, predicted_label_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a8a347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's macro F1 by century: 0.6191372111423143\n",
      "Most freq baseline by century: 0.08370461881799705\n",
      "Random baseline by century: 0.15942687213082418\n"
     ]
    }
   ],
   "source": [
    "true_century_labels = [(idx_to_decade[x] // 100) * 100 for x in true_labels]\n",
    "predicted_century_labels = [(idx_to_decade[x] // 100) * 100 for x in predicted_label_indices]\n",
    "century_f1_predictions = f1_score(true_century_labels, predicted_century_labels, average='macro')\n",
    "# print(collections.Counter(true_century_labels))\n",
    "# print(collections.Counter(predicted_century_labels))\n",
    "print(\"Model's macro F1 by century:\", century_f1_predictions)\n",
    "\n",
    "most_freq_century_baseline = [(idx_to_decade[x] // 100) * 100 for x in most_freq_baseline]\n",
    "random_century_baseline = [(idx_to_decade[x] // 100) * 100 for x in random_baseline]\n",
    "print(\"Most freq baseline by century:\", f1_score(true_century_labels, most_freq_century_baseline, average='macro'))\n",
    "print(\"Random baseline by century:\", f1_score(true_century_labels, random_century_baseline, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158e20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's book F1: 0.3519307857084939\n",
      "Model's movie F1: 0.04335903357313307\n"
     ]
    }
   ],
   "source": [
    "# split test dataset into books and movies and predictions to see if f1 score differs there\n",
    "book_indices = []\n",
    "movie_indices = []\n",
    "for i, sample in enumerate(test_dataset.samples):\n",
    "    if \"book\" in sample:\n",
    "        if sample[\"book\"].item():\n",
    "            book_indices.append(i)\n",
    "        else:\n",
    "            movie_indices.append(i)\n",
    "\n",
    "true_book_labels = [true_labels[x] for x in book_indices]\n",
    "true_movie_labels = [true_labels[x] for x in movie_indices]\n",
    "\n",
    "predicted_book_labels = [predicted_label_indices[x] for x in book_indices]\n",
    "predicted_movie_labels = [predicted_label_indices[x] for x in movie_indices]\n",
    "\n",
    "book_f1_predictions = f1_score(true_book_labels, predicted_book_labels, average='macro')\n",
    "print(\"Model's book F1:\", book_f1_predictions)\n",
    "\n",
    "movie_f1_predictions = f1_score(true_movie_labels, predicted_movie_labels, average='macro')\n",
    "print(\"Model's movie F1:\", movie_f1_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d98f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_decade(model, tokenizer, id2label, text, device=\"cpu\", max_len=128):\n",
    "    model.eval()\n",
    "\n",
    "    # preprocess text\n",
    "    text = str(text).lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # CORN decode\n",
    "    probs = torch.sigmoid(logits)\n",
    "    pred_id = int(torch.sum(probs > 0.5, dim=1).item())\n",
    "\n",
    "    # map to decade\n",
    "    decade = id2label[pred_id]\n",
    "    return decade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7872cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted decade: 1910\n"
     ]
    }
   ],
   "source": [
    "text = \"“Hateful day when I received life! Accursed creator! Why did you form a monster so hideous that even you turned from me in disgust? God, in pity, made man beautiful and alluring, after his own image; but my form is a filthy type of yours, more horrid even from the very resemlance. Satan had his companions, fellow-devils, to admire and encourage him; but I am solitary and abhorred.'\"\n",
    "predicted_decade = predict_decade(model, AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"), idx_to_decade, text, device)\n",
    "print(\"Predicted decade:\", predicted_decade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5264f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted decade: 1900\n"
     ]
    }
   ],
   "source": [
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife\"\n",
    "predicted_decade = predict_decade(model, AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"), idx_to_decade, text, device)\n",
    "print(\"Predicted decade:\", predicted_decade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b52a492e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted decade: 1990\n"
     ]
    }
   ],
   "source": [
    "text = \"Look, I didn’t want to be a half-blood. If you’re reading this because you think you might be one, my advice is: close this book right now. Believe whatever lie your mom or dad told you about your birth, and try to lead a normal life.\"\n",
    "predicted_decade = predict_decade(model, AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"), idx_to_decade, text, device)\n",
    "print(\"Predicted decade:\", predicted_decade)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
